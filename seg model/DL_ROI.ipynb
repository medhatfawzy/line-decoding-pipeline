{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "These are the important imports and functions used in the detection part of the project\n",
    "assuming the other libraries are already installed and imported like cv2, numpy, etc.\n",
    "'''\n",
    "from fastai.vision.all import *\n",
    "from fastai.metrics import accuracy\n",
    "from fastseg import MobileV3Small\n",
    "import torch\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The functions needed for the detection part\n",
    "def get_pred_for_mobilenet(model, img_array): # this is to make sure that the model is in the GPU\n",
    "    with torch.no_grad():\n",
    "        # image_tensor = np.expand_dims(img_array, -1).transpose(2, 0, 1).astype('float32') / 255\n",
    "        image_tensor = img_array.transpose(2, 0, 1).astype('float32') / 255\n",
    "        x_tensor = torch.from_numpy(image_tensor).to(\"cuda\").unsqueeze(0)\n",
    "        model_output = F.softmax(model.forward(x_tensor), dim=1).cpu().numpy()\n",
    "    return model_output\n",
    "\n",
    "def extract_lane_boundaries(left_mask, right_mask, threshold=0.3):\n",
    "    left_boundary = (left_mask > threshold).astype(np.uint8)\n",
    "    right_boundary = (right_mask > threshold).astype(np.uint8)\n",
    "\n",
    "    contours, _ = cv2.findContours(left_boundary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if len(contours) > 0:\n",
    "        left_boundary = cv2.drawContours(left_boundary, contours, -1, (1), thickness=cv2.FILLED)\n",
    "\n",
    "    contours, _ = cv2.findContours(right_boundary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if len(contours) > 0:\n",
    "        right_boundary = cv2.drawContours(right_boundary, contours, -1, (1), thickness=cv2.FILLED)\n",
    "\n",
    "    return left_boundary, right_boundary\n",
    "\n",
    "def lane_detection_pipeline(img, model_path=\"seg_model.pth\"):\n",
    "    # Load the model\n",
    "    model = torch.load(model_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.GaussianBlur(img, (5, 5), 0)\n",
    "\n",
    "    # Get predictions\n",
    "    back, left, right = get_pred_for_mobilenet(model, img)[0]\n",
    "\n",
    "    # Extract left and right boundaries\n",
    "    left_boundary, right_boundary = extract_lane_boundaries(left, right)\n",
    "\n",
    "    # Create ROI mask\n",
    "    # roi_mask = (left_boundary | right_boundary).astype(np.uint8)\n",
    "\n",
    "    # # Create a copy of the original image\n",
    "    # image_with_roi = np.copy(img)\n",
    "\n",
    "    # # Set all pixels outside the ROI to black\n",
    "    # image_with_roi[roi_mask == 0] = [0, 0, 0]\n",
    "\n",
    "    # # Overlay the left and right lane boundaries in red color\n",
    "    # image_with_roi[left_boundary > 0] = [255, 0, 0]\n",
    "    # image_with_roi[right_boundary > 0] = [0, 255, 0]\n",
    "\n",
    "    # Find the coordinates of the left and right boundaries\n",
    "    left_y, left_x = np.where(left_boundary > 0)\n",
    "    right_y, right_x = np.where(right_boundary > 0)\n",
    "\n",
    "    # Create a black background mask with the same dimensions as the image\n",
    "    mask = np.zeros_like(img, dtype=np.uint8)\n",
    "\n",
    "    # Create a polygon for the left boundary\n",
    "    left_polygon = np.column_stack((left_x, left_y))\n",
    "\n",
    "    # Create a polygon for the right boundary (reverse the order to close the gap)\n",
    "    right_polygon = np.column_stack((right_x[::-1], right_y[::-1]))\n",
    "\n",
    "    # Combine the left and right polygons to fill the region between them\n",
    "    polygon_points = np.vstack((left_polygon, right_polygon))\n",
    "\n",
    "    # Use fillPoly to fill the region between the left and right boundaries\n",
    "    try:\n",
    "        cv2.fillPoly(mask, [polygon_points], (255, 255, 255))\n",
    "    except Exception as e:\n",
    "        print(f\"Error in cv2.fillPoly: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Bitwise-AND your original image and the mask\n",
    "    result = cv2.bitwise_and(img, mask)\n",
    "\n",
    "    # Save the resulting image\n",
    "    # cv2.imwrite(save_path, result)\n",
    "    return result, left, right\n",
    "\n",
    "    # return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warp_img(img):\n",
    "    # https://nikolasent.github.io/opencv/2017/05/07/Bird's-Eye-View-Transformation.html\n",
    "    IMAGE_H = img.shape[0]\n",
    "    IMAGE_W = img.shape[1]\n",
    "    src = np.float32(\n",
    "        [[0, IMAGE_H], [IMAGE_W, IMAGE_H], [0, IMAGE_H // 10], [IMAGE_W, IMAGE_H // 10]]\n",
    "    )\n",
    "    dst = np.float32(\n",
    "        [[IMAGE_W // 2.8, IMAGE_H], [IMAGE_W // 1.8, IMAGE_H], [0, 0], [IMAGE_W, 0]]\n",
    "    )\n",
    "    img = img[int(IMAGE_H // 2):IMAGE_H, :]  # Apply np slicing for ROI crop\n",
    "    M = cv2.getPerspectiveTransform(src, dst)  # The transformation matrix\n",
    "    img = cv2.warpPerspective(img, M, (IMAGE_W, IMAGE_H))  # Image warping\n",
    "    img = img[\n",
    "        int(IMAGE_H // 10) : int(IMAGE_H // 1.3),\n",
    "        int(IMAGE_W // 3) : int(IMAGE_W // 1.7),\n",
    "    ]\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_imgs(imgs, titles):\n",
    "    rows = int(np.ceil(len(imgs) / 3 ))\n",
    "    cols = 3\n",
    "    figsize = (cols * 5, rows * 4)\n",
    "    _, axs = plt.subplots(rows, cols, figsize=figsize)\n",
    "    for img, title, ax in zip(imgs, titles, axs.flatten()):\n",
    "        ax.imshow(img, cmap=\"gray\")\n",
    "        ax.set_title(title)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_process_image(img):\n",
    "    img = warp_img(img)\n",
    "    img_warp = img.copy()\n",
    "    img = gaussian_blur(img)\n",
    "    # img = harris_corner(img)\n",
    "    img = canny_edge(img)\n",
    "    img_canny = img.copy()\n",
    "    img = dilation(img)\n",
    "    img_w, img_h = img.shape[1], img.shape[0]\n",
    "\n",
    "    lines = detect_hough_lines(img)\n",
    "    if lines is None:\n",
    "        return img, np.zeros((img_h, img_w), dtype=np.uint8)\n",
    "\n",
    "    img_hou = np.zeros((img_h, img_w), dtype=np.uint8)\n",
    "    img_hou = draw_hough_lines(img_hou, lines)\n",
    "\n",
    "    rects, bottom_center_rect_idx = detect_correct_mark(img_hou)\n",
    "\n",
    "    throttle, steer = map_values(rects[bottom_center_rect_idx], img_hou)\n",
    "    # return throttle, steer\n",
    "\n",
    "    img_hou = cv2.cvtColor(img_hou, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    center_bottom = (img_hou.shape[1] // 2, img_hou.shape[0] // 1.5)\n",
    "    cv2.circle(\n",
    "        img_hou,\n",
    "        center=tuple(map(int, center_bottom)),\n",
    "        radius=50,\n",
    "        color=(0, 0, 255),\n",
    "        thickness=10,\n",
    "    )\n",
    "    for i, rect in enumerate(rects):\n",
    "        img_hou = draw_rectangle_features(img_hou, rect, throttle, steer, idx=i)\n",
    "    \n",
    "    cv2.putText(\n",
    "        img_hou,\n",
    "        text=f\"{bottom_center_rect_idx} SELECTED\",\n",
    "        org=(0, 50),\n",
    "        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        fontScale=1,\n",
    "        color=(0, 255, 255),\n",
    "        thickness=3,\n",
    "    )\n",
    "    \n",
    "    return img_warp, img_hou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(\"../imgs/output-3.mp4\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\") or ret == False:\n",
    "        break\n",
    "    result, left, right = lane_detection_pipeline(frame)\n",
    "    result_warped, result_hou = show_process_image(result)\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    cv2.imshow(\"Lane\", result)\n",
    "    cv2.imshow(\"Warped\", result_hou)\n",
    "    # time.sleep(0.1)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
